{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 13:45:14.730602: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 13:45:14.730638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 13:45:14.731863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-12 13:45:14.737808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 13:45:15.385345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import joblib as jb\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from math import sqrt, atan2\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import math\n",
    "import difflib\n",
    "import collections\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten, Dropout, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "folder ='S1' # change the folder as per requirement\n",
    "feat_train_amp = pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/X_train.csv'))\n",
    "label_train_amp=pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/y_train.csv'))\n",
    "feat_test_amp = pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/X_test.csv'))\n",
    "label_test_amp = pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/y_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/vijay/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# select 100 best\n",
    "selector = SelectKBest(f_classif, k=100)\n",
    "feat_train_amp_100 = selector.fit_transform(feat_train_amp, label_train_amp)\n",
    "feat_test_amp_100 = selector.fit_transform(feat_test_amp, label_test_amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label using one hot encoding \n",
    "label_train_amp = pd.DataFrame(label_train_amp)\n",
    "onehot_encoder = OneHotEncoder()\n",
    "label_train_amp_one= onehot_encoder.fit_transform(label_train_amp)\n",
    "label_test_amp_one= onehot_encoder.fit_transform(label_test_amp)\n",
    "label_train_amp_one = label_train_amp_one.toarray()\n",
    "label_test_amp_one = label_test_amp_one.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaoe input dimension\n",
    "feat_train_amp_10X10=np.reshape(feat_train_amp_100,(feat_train_amp_100.shape[0],10,10))\n",
    "feat_test_amp_10X10=np.reshape(feat_test_amp_100,(feat_test_amp_100.shape[0],10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train test and validation.\n",
    "\n",
    "X_train, X_val, y_train, y_val=train_test_split(feat_train_amp_10X10, label_train_amp_one, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Attention layer.\n",
    "class AttenLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_state, **kw):\n",
    "        super(AttenLayer, self).__init__(**kw)\n",
    "        self.num_state = num_state\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight('kernel', shape=[input_shape[-1], self.num_state])\n",
    "        self.bias = self.add_weight('bias', shape=[self.num_state])\n",
    "        self.prob_kernel = self.add_weight('prob_kernel', shape=[self.num_state])\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        atten_state = tf.tanh(tf.tensordot(input_tensor, self.kernel, axes=1) + self.bias)\n",
    "        logits = tf.tensordot(atten_state, self.prob_kernel, axes=1)\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        weighted_feature = tf.reduce_sum(tf.multiply(input_tensor, tf.expand_dims(prob, -1)), axis=1)\n",
    "        return weighted_feature\n",
    "\n",
    "    # for saving the model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_state': self.num_state,})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Bi-LSTM model\n",
    "class CSIModelConfig:\n",
    "    def __init__(self, win_len=1000, step=200, thrshd=0.6, downsample=2):\n",
    "        self._win_len = win_len\n",
    "        self._step = step\n",
    "        self._thrshd = thrshd\n",
    "        self._labels = (\"Forward\", \"Looking Down\", \"Looking Up\", \"Looking Left\", \"Looking Right\", \"Nodding\", \"Shaking\")\n",
    "        self._downsample = downsample\n",
    "\n",
    "    def build_model(self, n_unit_lstm=200, n_unit_atten=400,l1_reg=0.01):\n",
    "        \"\"\"\n",
    "        Returns the Tensorflow Model which uses AttenLayer\n",
    "        \"\"\"\n",
    "        if self._downsample > 1:\n",
    "            length = len(np.ones((self._win_len,))[::self._downsample])\n",
    "            x_in = tf.keras.Input(shape=(length, 10))\n",
    "        else:\n",
    "            x_in = tf.keras.Input(shape=(self._win_len, 10))\n",
    "        \n",
    "        x_tensor = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=n_unit_lstm, return_sequences=True,kernel_regularizer=tf.keras.regularizers.l1(l1_reg)))(x_in)\n",
    "        x_tensor = AttenLayer(n_unit_atten)(x_tensor)\n",
    "        pred = tf.keras.layers.Dense(len(self._labels), activation='softmax')(x_tensor)\n",
    "        model = tf.keras.Model(inputs=x_in, outputs=pred)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 10, 10)]          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 10, 800)           1315200   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " atten_layer_1 (AttenLayer)  (None, 800)               320800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 5607      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1641607 (6.26 MB)\n",
      "Trainable params: 1641607 (6.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "530/531 [============================>.] - ETA: 0s - loss: 1.5652 - accuracy: 0.3904INFO:tensorflow:Assets written to: /home/vijay/paper_jc/Neurips_raw_csi_data/code/model/bilstm_S1.sav/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/vijay/paper_jc/Neurips_raw_csi_data/code/model/bilstm_S1.sav/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 33s 59ms/step - loss: 1.5652 - accuracy: 0.3905 - val_loss: 1.5563 - val_accuracy: 0.4230\n",
      "Epoch 2/2\n",
      "531/531 [==============================] - ETA: 0s - loss: 1.4732 - accuracy: 0.4421INFO:tensorflow:Assets written to: /home/vijay/paper_jc/Neurips_raw_csi_data/code/model/bilstm_S1.sav/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/vijay/paper_jc/Neurips_raw_csi_data/code/model/bilstm_S1.sav/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 31s 58ms/step - loss: 1.4732 - accuracy: 0.4421 - val_loss: 1.4319 - val_accuracy: 0.4277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f53b8492590>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = CSIModelConfig(win_len=10, step=250, thrshd=0.8, downsample=1)\n",
    "model = cfg.build_model(n_unit_lstm=400, n_unit_atten=400, l1_reg=0.0)\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=56, epochs=30,\n",
    "        validation_data=[X_val, y_val],\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(os.path.abspath(f'./model/bilstm_{folder}.sav'),\n",
    "                                                monitor='val_accuracy',\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=False)\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_path = os.path.abspath(f'./model/bilstm_{folder}.sav')\n",
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 5s 16ms/step\n",
      "F1-score 0.43790426908150065\n",
      "Accuracy 0.43790426908150065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "y_pred= loaded_model.predict(feat_test_amp_10X10)\n",
    "y_predicted= np.argmax(y_pred, axis=1)\n",
    "ground_truth= np.argmax(label_test_amp_one, axis=1)\n",
    "re= f1_score(y_predicted, ground_truth,average='micro')\n",
    "print(\"F1-score\", re)\n",
    "print(\"Accuracy\", accuracy_score(y_predicted, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 -fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "cfg = CSIModelConfig(win_len=10, step=250, thrshd=0.8, downsample=1)\n",
    "model = cfg.build_model(n_unit_lstm=400, n_unit_atten=400, l1_reg=0.0)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy'])\n",
    "model.summary()\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_, X_valid_ = X_train[train_index], X_train[test_index]\n",
    "    Y_train_, Y_valid_ = y_train[train_index], y_train[test_index]\n",
    "    model.fit(\n",
    "            X_train_,\n",
    "            Y_train_,\n",
    "            batch_size=56, epochs=30,\n",
    "            validation_data=(X_valid_, Y_valid_),\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.ModelCheckpoint('best_atten.hdf5',\n",
    "                                                    monitor='val_accuracy',\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=False)\n",
    "                ])  \n",
    "    y_pred = model.predict(X_valid_)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(np.argmax(Y_valid_, axis=1), y_pred_classes)\n",
    "    f1 = f1_score(np.argmax(Y_valid_, axis=1), y_pred_classes, average='weighted')\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    print(f1_scores)\n",
    "\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
