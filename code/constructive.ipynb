{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import sqrt, atan2\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import scipy.signal as signal\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "folder ='S1' # change the folder as per requirement\n",
    "feat_train_amp = pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/X_train.csv'))\n",
    "label_train_amp=pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/y_train.csv'))\n",
    "feat_test_amp = pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/X_test.csv'))\n",
    "label_test_amp = pd.read_csv(os.path.abspath(f'./training_testing_data/{folder}/y_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode label \n",
    "label_train_amp.headlabel.replace(('Forward', 'Looking Up', 'Nodding', 'Looking Down', 'Shaking','Looking Left', 'Looking Right' ), (1, 2,3, 4,5,6,7), inplace=True)\n",
    "label_test_amp.headlabel.replace(('Forward', 'Looking Up', 'Nodding', 'Looking Down', 'Shaking','Looking Left', 'Looking Right' ), (1, 2,3, 4,5,6,7), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce feature dimension \n",
    "selector = SelectKBest(f_classif, k=100)\n",
    "feat_train_amp_100 = selector.fit_transform(feat_train_amp, label_train_amp)\n",
    "feat_test_amp_100 = selector.fit_transform(feat_test_amp, label_test_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_label, test_label = train_test_split(feat_train_amp_100, label_train_amp, test_size=0.30, random_state=42)\n",
    "train_data, val_data, train_label, val_label= train_test_split(train_data, train_label, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasperNetwork(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(CasperNetwork, self).__init__()\n",
    "        \n",
    "        # create input layer with a randomly initialized weight\n",
    "        self.Initial = torch.nn.Linear(n_input, n_output) \n",
    "        self.Initial.weight.data = self.initialize_weights(n_input, n_output, \n",
    "                                                    -0.7, 0.7)\n",
    "        \n",
    "        # This list contains all the input connections to hidden neurons\n",
    "        self.old_input_neurons = nn.ModuleList([]) \n",
    "        # This list contains all the ouput connections from previous neurons\n",
    "        self.old_output_neurons = nn.ModuleList([]) \n",
    "        self.n_neurons = 0\n",
    "        \n",
    "        self.input_size = n_input\n",
    "        self.output_size = n_output\n",
    "\n",
    "        # initialize casper network with no hidden neurons\n",
    "        self.L1 = None\n",
    "        self.L2 = None\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(n_input, n_output) \n",
    "        self.output_layer.weight.data = self.initialize_weights(n_input, \n",
    "                                                                n_output, \n",
    "                                                                -0.7, 0.7)\n",
    "    def forward(self, x):\n",
    "        # calculate output from input layer\n",
    "        out = x\n",
    "\n",
    "        # if there are no hidden nerons, simply return the output\n",
    "        if len(self.old_input_neurons) == 0:\n",
    "            if self.L1 == None:\n",
    "                \n",
    "                # if no neurons has been inserted, simply pass input to ouput\n",
    "                return self.Initial(x)\n",
    "            \n",
    "            # if there is a single hidden neuron, \n",
    "            # then add its output to the output layer\n",
    "            else:\n",
    "                \n",
    "                temp = torch.tanh(self.L1(x))\n",
    "                temp = torch.tanh(self.L2(temp))\n",
    "                out = torch.cat((out, temp), 1)\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            # if there is more than 1 hidden neuron, loop through the list of\n",
    "            # casper neurons and add their output to the final output layer\n",
    "            for index in range(0, len(self.old_input_neurons)):\n",
    "                \n",
    "                # calculate the inputs to the single casper neuron\n",
    "                previous = self.old_input_neurons[index](x)\n",
    "                \n",
    "                # concatenate the output from the single casper neuron to the \n",
    "                # outputs of all previous neurons\n",
    "                out = torch.cat((out, self.old_output_neurons[index]\n",
    "                                (torch.tanh(previous))), 1) \n",
    "\n",
    "                # concatenate the single neuron to the input \n",
    "                x = torch.cat((x, previous), 1)\n",
    "                \n",
    "            # calculate the output from the most recent casper neuron \n",
    "            # add them to the final output layer\n",
    "            new_neuron_input = torch.tanh(self.L1(x))\n",
    "            new_neuron_output = torch.tanh(self.L2(new_neuron_input))\n",
    "            out = torch.cat((out, new_neuron_output), 1)\n",
    "          \n",
    "        return self.output_layer(out)\n",
    "    \n",
    "     \n",
    "    # adds new casper neuron to the network, which would be 2 linear layers\n",
    "    # layer1: (input, 1) layer2: (1, output), \n",
    "    # the flow would be inputs -> 1 neuron -> output\n",
    "    def add_layer(self):\n",
    "        self.n_neurons += 1\n",
    "        \n",
    "        # concatenate all outputs from the hidden neurons and original input\n",
    "        # to go into the output neurons\n",
    "        previous_weights = self.output_layer.weight.data\n",
    "        total_outputs = self.n_neurons + self.input_size\n",
    "        self.output_layer = torch.nn.Linear(total_outputs , self.output_size) \n",
    "        \n",
    "        # copy over the previsouly learnt weights, and initialize random\n",
    "        # weight values for new neurons\n",
    "        self.output_layer.weight.data = self.copy_initialize_weights(\n",
    "                                                            previous_weights, \n",
    "                                                            total_outputs, \n",
    "                                                            self.output_size, \n",
    "                                                            -0.1, 0.1)\n",
    "        \n",
    "        # create the layers for the new casper neuron\n",
    "        new_layer_in = torch.nn.Linear(self.input_size + self.n_neurons - 1, 1)\n",
    "        # we pass it through another neuron in order to create an per neuron\n",
    "        # learning rate for the final layer\n",
    "        new_layer_out = torch.nn.Linear(1, 1)\n",
    "        \n",
    "        total_inputs = self.input_size + self.n_neurons - 1\n",
    "        \n",
    "        # initialize weights\n",
    "        new_layer_in.weight.data = self.initialize_weights(total_inputs, \n",
    "                                                         1, -0.1, 0.1)\n",
    "        new_layer_out.weight.data = self.initialize_weights(1, 1, -0.1, 0.1)\n",
    "        \n",
    "        # assign the layers to the network\n",
    "        if self.L1 == None and self.L2 == None:\n",
    "            self.L1 = new_layer_in\n",
    "            self.L2 = new_layer_out\n",
    "        \n",
    "        else:\n",
    "            self.old_input_neurons.append(self.L1)\n",
    "            self.old_output_neurons.append(self.L2)\n",
    "            self.L1 = new_layer_in\n",
    "            self.L2 = new_layer_out\n",
    "         \n",
    "        \n",
    "        \n",
    "    # create a list of weights for initialization\n",
    "    def initialize_weights(self, n_input, n_output, lower, upper):\n",
    "        final = []\n",
    "        for inputs in range(0, n_output):\n",
    "            weights = []\n",
    "            for value in range(0, n_input):\n",
    "                weights.append(np.random.uniform(lower, upper))\n",
    "            final.append(weights)\n",
    "        return torch.Tensor(final)\n",
    "    \n",
    "    # Creates a list of weights for initialization, but also copies over the \n",
    "    # previous weights avoiding the need to relearn\n",
    "    def copy_initialize_weights(self, previous_weight, n_input, n_output, \n",
    "                                lower, upper):\n",
    "        final = []\n",
    "        \n",
    "        for row in range(0, n_output):\n",
    "            weights = []\n",
    "            \n",
    "            for value in range(0, len(previous_weight[row])):\n",
    "                weights.append(previous_weight[row][value])\n",
    "                    \n",
    "            for new_weight in range(0, n_input - len(previous_weight[row])):\n",
    "                weights.append(np.random.uniform(lower, upper))\n",
    "\n",
    "            final.append(weights)\n",
    "        return torch.Tensor(final)\n",
    "    \n",
    "    \n",
    "    def applyWeightDecay(self, decay):\n",
    "        self.Initial.weight.data *= decay\n",
    "        \n",
    "        if self.L1 != None:\n",
    "            self.L1.weight.data *= decay\n",
    "            self.L2.weight.data *= decay\n",
    "            \n",
    "        if len(self.old_input_neurons) != 0:\n",
    "            for layers in self.old_input_neurons:\n",
    "                layers.weight.data *= decay\n",
    "                \n",
    "            for layers in self.old_output_neurons:\n",
    "                layers.weight.data *= decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.Tensor(train_data).float())\n",
    "Y = Variable(torch.Tensor(train_label.values).long())\n",
    "VX = Variable(torch.Tensor(val_data).float())\n",
    "VY = Variable(torch.Tensor(val_label.values).long())\n",
    "Y=Y.flatten()-1\n",
    "VY=VY.flatten()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1360\n",
    "output_neurons = 7\n",
    "input_neurons =100\n",
    "weight_decay = 0.998\n",
    "threshold=0.5\n",
    "\n",
    "L1 = 0.005\n",
    "L2 = 0.001\n",
    "L3 = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CasperNetwork(input_neurons, output_neurons)\n",
    "# net=net.to(device)\n",
    "\n",
    "\n",
    "# define loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "# define optimiser with per layer learning rates\n",
    "# optimiser without any hidden neurons\n",
    "optimiser = optim.Rprop([\n",
    "                {'params': net.Initial.parameters(), 'lr' : L1},\n",
    "                {'params': net.output_layer.parameters()},\n",
    "                {'params': net.old_input_neurons.parameters()},\n",
    "                {'params': net.old_output_neurons.parameters()}], \n",
    "                lr = L3, etas = (0.5, 1.2), step_sizes=(1e-06, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "val_loss=[]\n",
    "\n",
    "\n",
    "previous_loss = None\n",
    "\n",
    "# train a neural network\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(X.size()[0])\n",
    "    temp_loss=0\n",
    "    idx=0\n",
    "#     torch.cuda.empty_cache()\n",
    "    for i in range(0,X.size()[0], batch_size):\n",
    "        idx=idx+1\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X[indices], Y[indices]\n",
    "#         batch_x=batch_x.to(device)\n",
    "#         batch_y=batch_y.to(device)\n",
    "#         net.to(device)\n",
    "    # Perform forward pass: compute predicted y by passing x to the model.\n",
    "        Y_pred = net(batch_x)\n",
    "    #     Y_pred=(Y_pred)\n",
    "    #     print(Y_pred)\n",
    "    #     print(Y_pred.shape)\n",
    "    #     print(Y.shape)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_func(Y_pred, batch_y)\n",
    "\n",
    "        temp_loss=temp_loss+loss\n",
    "        \n",
    "    # Clear the gradients before running the backward pass.\n",
    "    net.zero_grad()\n",
    "    \n",
    "    ll=(temp_loss)/idx\n",
    "\n",
    "    # Perform backward pass\n",
    "    ll.backward()\n",
    "\n",
    "    # Calling the step function on an Optimiser makes an update to its\n",
    "    # parameters\n",
    "    optimiser.step()\n",
    "    net.applyWeightDecay(weight_decay)\n",
    "    \n",
    "    val_temp=0\n",
    "    \n",
    "    idxv=0\n",
    "    permutation_v = torch.randperm(VX.size()[0])\n",
    "    for i in range(0,VX.size()[0], batch_size):\n",
    "        idxv=idxv+1\n",
    "        net.eval()\n",
    "        indices_v = permutation_v[i:i+batch_size]\n",
    "        batch_x_val, batch_y_val = VX[indices_v], VY[indices_v]\n",
    "#         batch_x_val=batch_x_val.to(device)\n",
    "#         batch_y_val=batch_y_val.to(device)\n",
    "#         net.to(device)\n",
    "    # Perform forward pass: compute predicted y by passing x to the model.\n",
    "        Y_pred_v = net(batch_x_val)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_func(Y_pred_v, batch_y_val)\n",
    "\n",
    "        val_temp=val_temp+loss\n",
    "    \n",
    "    all_losses.append((temp_loss)/idx)\n",
    "    val_loss.append((val_temp)/idxv)\n",
    "    # print progress\n",
    "    if epoch % 40 == 0:\n",
    "        # convert three-column predicted Y values to one column for comparison\n",
    "        _, predicted = torch.max(Y_pred, 1)\n",
    "        _, predicted_v = torch.max(Y_pred_v, 1)\n",
    "#         predicted=predicted.to(device)\n",
    "#             print(predicted)\n",
    "\n",
    "        # calculate and print accuracy\n",
    "        total = predicted.size(0)\n",
    "        correct = predicted.cpu().data.numpy() == batch_y.cpu().data.numpy()\n",
    "        \n",
    "        total_v = predicted_v.size(0)\n",
    "        correct_v = predicted_v.cpu().data.numpy() == batch_y_val.cpu().data.numpy()\n",
    "        \n",
    "#         print(correct)\n",
    "        print('Epoch [%d/%d] Train Loss: %.4f  Accuracy: %.2f %%'\n",
    "              % (epoch + 1, num_epochs, all_losses[-1], 100 * sum(correct)/total))\n",
    "        print('Epoch [%d/%d] Val Loss: %.4f  Accuracy: %.2f %%'\n",
    "              % (epoch + 1, num_epochs, val_loss[-1], 100 * sum(correct_v)/total_v))\n",
    "    \n",
    "\n",
    "        # if the rate to which the loss value decreases slows beyond a certain\n",
    "        # threshold, then add a casper neuron\n",
    "        if (previous_loss != None and previous_loss > all_losses[-1] and \n",
    "                                    previous_loss - all_losses[-1] < threshold) :\n",
    "\n",
    "            net.add_layer()\n",
    "\n",
    "            # adding custom learning rates to hidden neurons\n",
    "            optimiser = optim.Rprop([\n",
    "                {'params': net.Initial.parameters()},\n",
    "                {'params': net.old_input_neurons.parameters()},\n",
    "                {'params': net.old_output_neurons.parameters()},\n",
    "                {'params': net.output_layer.parameters()},\n",
    "                {'params': net.L1.parameters(), 'lr': L1},\n",
    "                {'params': net.L2.parameters(), 'lr': L2}], \n",
    "                lr = L3, etas = (0.5, 1.2), step_sizes=(1e-06, 50))\n",
    "\n",
    "        previous_loss = all_losses[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 45.22 %\n",
      "F1-score: 0.42898622256706176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# create Tensors to hold inputs and outputs, and wrap them in Variables,\n",
    "# as Torch only trains neural network on Variables\n",
    "X_test = Variable(torch.Tensor(test_data).float())\n",
    "Y_test = Variable(torch.Tensor(test_label.values).long())\n",
    "# X_test=X_test.to(device)\n",
    "# Y_test=Y_test.to(device)\n",
    "Y_test=Y_test.flatten()-1\n",
    "\n",
    "# test the neural network using testing data\n",
    "Y_pred_test = net(X_test)\n",
    "\n",
    "# get prediction\n",
    "# convert three-column predicted Y values to one column for comparison\n",
    "_, predicted_test = torch.max(Y_pred_test, 1)\n",
    "\n",
    "# calculate accuracy\n",
    "total_test = predicted_test.size(0)\n",
    "correct_test = sum(predicted_test.cpu().data.numpy() == Y_test.cpu().data.numpy())\n",
    "\n",
    "print('Testing Accuracy: %.2f %%' % (100 * correct_test / total_test))\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(Y_test, predicted_test, average='weighted')\n",
    "print('F1-score:', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
